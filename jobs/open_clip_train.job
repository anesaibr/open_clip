#!/bin/bash

#SBATCH --partition=gpu_h100            # or gpu_a100 (cpu:72)
#SBATCH --gpus=2                        # Max GPUs per node (4)
#SBATCH --job-name=TrainingCLIP
#SBATCH --ntasks=1                      # 1 task (total)
#SBATCH --cpus-per-task=16             # ~16-18 CPUs per GPU (64/4 -> gpu_h100 or 72/4 -> gpu_a100)
#SBATCH --time=07:40:00
#SBATCH --output=output/success/out-%x.%A.out
#SBATCH --error=output/error/out-%x.%A.err

module purge
module load 2023
module load Anaconda3/2023.07-2

cd $HOME/open_clip/

source activate memory_clip

# export LD_LIBRARY_PATH=/home/aibrahimi/.conda/envs/memory_clip/lib/python3.11/site-packages/nvidia/nccl/lib:$LD_LIBRARY_PATH
# NCCL Environment Variables
# export NCCL_IB_TIMEOUT=24 # Increase from default 22 to 24 for InfiniBand
# export NCCL_DEBUG=WARN  # Not "INFO", Critical for error detection.
# export NCCL_DEBUG_SUBSYS=COLL,GRAPH # Filters logs to collective operations and communication graphs.
# export NCCL_LAUNCH_MODE=GROUP # Groups NCCL operations for efficiency.
# export NCCL_ASYNC_ERROR_HANDLING=1 # Enable async error handling
# # export NCCL_P2P_DISABLE=1
# # export NCCL_IB_DISABLE=1
# export TORCH_DISTRIBUTED_DEBUG=DETAIL # Detailed PyTorch distributed logs
# export NCCL_COLLNET_ENABLE=0 # Disables CollNet (collective network optimizations), which can cause hangs in some topologies.
# export NCCL_SHM_DISABLE=1 #Disables shared memory for NCCL, forcing it to use network/sockets. Helps isolate shared memory bugs.
# export CUDA_LAUNCH_BLOCKING=1


echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
echo "SLURM_LOCAL_RANK: $SLURM_LOCAL_RANK"
echo "SLURM_PROCID: $SLURM_PROCID"
echo "WORLD_SIZE: $WORLD_SIZE"


# salloc -w gcn59 -N 1 -n 1 -t 00:30:00 -p gpu_a100
#You could request 2 GPUs (--gpus-per-node=2) without increasing the price.
#--train-num-sample => total_samples=$(grep -rh "successes" mscoco/*_stats.json | awk '{sum += $2} END {print sum}')
# --resume="/projects/0/prjs1465/logs/2025_03_25-00_51_04-model_ViT-B-16-lr_0.001-b_128-j_8-p_amp_bf16/checkpoints/epoch_1.pt" \ #FIXME: resume training from checkpoint
torchrun --nproc_per_node 2 -m src.open_clip_train.main -- \
    --save-frequency 1 \
    --train-data="/scratch-shared/mscoco/mscoco/{00000..00047}.tar" \
    --val-data="/scratch-shared/mscoco/mscoco/{00048..00059}.tar"  \
    --dataset-type=webdataset \
    --train-num-samples 591753 \
    --use-memory \
    --warmup 1000 \
    --batch-size=20  \
    --accum-freq 4 \
    --lr=5e-4 \
    --wd=0.1 \
    --epochs=10 \
    --workers=4 \
    --grad-checkpointing \
    --model ViT-B-16 \
    --precision amp_bf16 \
    --pretrained "openai" \
    --report-to wandb \
    --wandb-project-name "memory-open-clip" \
    --logs="/projects/0/prjs1465/logs" \
    --log-every-n-steps 100 

# torchrun --nproc_per_node 4 -m testing_dist